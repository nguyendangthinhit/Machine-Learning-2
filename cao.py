#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script cÃ o tiÃªu Ä‘á» tá»« cÃ¡c trang bÃ¡o
Äá»c links tá»« link.txt trong thÆ° má»¥c Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh vÃ  lÆ°u káº¿t quáº£ vÃ o data.json
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import sys
import os
from typing import Dict

class TitleScraper:
    """Class Ä‘á»ƒ cÃ o tiÃªu Ä‘á» tá»« cÃ¡c trang web"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_title(self, url: str, timeout: int = 10) -> str:
        """
        Láº¥y tiÃªu Ä‘á» tá»« má»™t URL
        
        Args:
            url: URL cáº§n láº¥y tiÃªu Ä‘á»
            timeout: Thá»i gian timeout (giÃ¢y)
            
        Returns:
            TiÃªu Ä‘á» cá»§a trang hoáº·c thÃ´ng bÃ¡o lá»—i
        """
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # TÃ¬m tiÃªu Ä‘á» - thá»­ nhiá»u cÃ¡ch
            title = None
            
            # CÃ¡ch 1: Tháº» <title>
            if soup.title:
                title = soup.title.get_text(strip=True)
            
            # CÃ¡ch 2: Meta property og:title (thÆ°á»ng chÃ­nh xÃ¡c hÆ¡n cho bÃ i bÃ¡o)
            if not title:
                og_title = soup.find('meta', property='og:title')
                if og_title and og_title.get('content'):
                    title = og_title.get('content').strip()
            
            # CÃ¡ch 3: Meta name twitter:title
            if not title:
                twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
                if twitter_title and twitter_title.get('content'):
                    title = twitter_title.get('content').strip()
            
            # CÃ¡ch 4: Tháº» h1 Ä‘áº§u tiÃªn
            if not title:
                h1 = soup.find('h1')
                if h1:
                    title = h1.get_text(strip=True)
            
            return title if title else "KhÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á»"
            
        except requests.exceptions.Timeout:
            return f"Lá»—i: Timeout khi truy cáº­p"
        except requests.exceptions.RequestException as e:
            return f"Lá»—i: {type(e).__name__}"
        except Exception as e:
            return f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {type(e).__name__}"
    
    def read_links(self, filename: str) -> list:
        """
        Äá»c danh sÃ¡ch links tá»« file
        
        Args:
            filename: ÄÆ°á»ng dáº«n file chá»©a links
            
        Returns:
            List cÃ¡c URLs
        """
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                # Äá»c tá»«ng dÃ²ng, loáº¡i bá» khoáº£ng tráº¯ng vÃ  dÃ²ng trá»‘ng
                links = [line.strip() for line in f if line.strip()]
            return links
        except FileNotFoundError:
            print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file {filename}")
            return []
        except Exception as e:
            print(f"âŒ Lá»—i khi Ä‘á»c file: {e}")
            return []
    
    def scrape_all(self, input_file: str, output_file: str, delay: float = 1.0):
        """
        CÃ o tiÃªu Ä‘á» tá»« táº¥t cáº£ links trong file vÃ  lÆ°u káº¿t quáº£
        
        Args:
            input_file: File chá»©a danh sÃ¡ch links
            output_file: File JSON Ä‘á»ƒ lÆ°u káº¿t quáº£
            delay: Thá»i gian delay giá»¯a cÃ¡c request (giÃ¢y)
        """
        print(f"ğŸ“‚ Äá»c links tá»«: {input_file}")
        links = self.read_links(input_file)
        
        if not links:
            print("âš ï¸  KhÃ´ng cÃ³ link nÃ o Ä‘á»ƒ cÃ o!")
            return
        
        print(f"ğŸ“ TÃ¬m tháº¥y {len(links)} links")
        print("ğŸš€ Báº¯t Ä‘áº§u cÃ o tiÃªu Ä‘á»...\n")
        
        results = {}
        
        for i, url in enumerate(links, 1):
            print(f"[{i}/{len(links)}] Äang cÃ o: {url}")
            
            title = self.get_title(url)
            results[url] = title
            
            # Hiá»ƒn thá»‹ tiÃªu Ä‘á» vá»›i Ä‘á»™ dÃ i giá»›i háº¡n
            display_title = title if len(title) <= 80 else title[:77] + "..."
            print(f"  âœ“ TiÃªu Ä‘á»: {display_title}\n")
            
            # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block (trá»« request cuá»‘i cÃ¹ng)
            if i < len(links):
                time.sleep(delay)
        
        # LÆ°u káº¿t quáº£ ra file JSON
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… ÄÃ£ lÆ°u káº¿t quáº£ vÃ o: {output_file}")
            print(f"âœ… Tá»•ng cá»™ng: {len(results)} tiÃªu Ä‘á»")
        except Exception as e:
            print(f"âŒ Lá»—i khi lÆ°u file: {e}")


def process_folder(scraper, folder_name, delay=1.0):
    """
    Xá»­ lÃ½ má»™t thÆ° má»¥c
    
    Args:
        scraper: TitleScraper instance
        folder_name: TÃªn thÆ° má»¥c cáº§n cÃ o
        delay: Thá»i gian delay giá»¯a cÃ¡c request
        
    Returns:
        True náº¿u thÃ nh cÃ´ng, False náº¿u cÃ³ lá»—i
    """
    # XÃ¢y dá»±ng Ä‘Æ°á»ng dáº«n file
    input_file = os.path.join(folder_name, 'links.txt')
    output_file = os.path.join(folder_name, 'data.json')
    
    # Kiá»ƒm tra file link.txt cÃ³ tá»“n táº¡i khÃ´ng
    if not os.path.isfile(input_file):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{input_file}'")
        print(f"ğŸ’¡ HÃ£y táº¡o file 'links.txt' trong thÆ° má»¥c '{folder_name}'")
        return False
    
    print("=" * 60)
    print(f"ğŸ¯ CÃ€O TIÃŠU Äá»€ - THÆ¯ Má»¤C: {folder_name.upper()}")
    print("=" * 60)
    
    # Cháº¡y scraper
    scraper.scrape_all(input_file, output_file, delay)
    
    print("=" * 60)
    print(f"âœ¨ HOÃ€N THÃ€NH THÆ¯ Má»¤C: {folder_name.upper()}")
    print("=" * 60)
    print()
    
    return True


def main():
    """HÃ m chÃ­nh"""
    # Kiá»ƒm tra tham sá»‘ dÃ²ng lá»‡nh
    if len(sys.argv) < 2:
        print("âŒ Lá»—i: Thiáº¿u tÃªn thÆ° má»¥c!")
        print("\nğŸ“– CÃ¡ch sá»­ dá»¥ng:")
        print("   python cao.py <tÃªn_thÆ°_má»¥c_1> [tÃªn_thÆ°_má»¥c_2] [tÃªn_thÆ°_má»¥c_3] ...")
        print("\nğŸ’¡ VÃ­ dá»¥:")
        print('   python cao.py "NÄT" "Q.Huy" "Thiá»‡n"')
        sys.exit(1)
    
    # Láº¥y danh sÃ¡ch thÆ° má»¥c tá»« tham sá»‘
    folder_names = sys.argv[1:]
    
    # Kiá»ƒm tra tá»«ng thÆ° má»¥c cÃ³ tá»“n táº¡i khÃ´ng
    invalid_folders = []
    valid_folders = []
    
    for folder_name in folder_names:
        if not os.path.isdir(folder_name):
            invalid_folders.append(folder_name)
        else:
            valid_folders.append(folder_name)
    
    # Náº¿u cÃ³ thÆ° má»¥c khÃ´ng tá»“n táº¡i, chá»‰ hiá»ƒn thá»‹ cáº£nh bÃ¡o
    if invalid_folders:
        print("âš ï¸  Cáº£nh bÃ¡o: CÃ¡c thÆ° má»¥c sau khÃ´ng tá»“n táº¡i (sáº½ bá» qua):")
        for folder in invalid_folders:
            print(f"   - {folder}")
        print()
    
    # Náº¿u khÃ´ng cÃ³ thÆ° má»¥c há»£p lá»‡ nÃ o
    if not valid_folders:
        print("âŒ KhÃ´ng cÃ³ thÆ° má»¥c há»£p lá»‡ nÃ o Ä‘á»ƒ xá»­ lÃ½!")
        print(f"\nğŸ’¡ CÃ¡c thÆ° má»¥c hiá»‡n cÃ³:")
        # Liá»‡t kÃª cÃ¡c thÆ° má»¥c con
        subdirs = [d for d in os.listdir('.') if os.path.isdir(d) and not d.startswith('.')]
        if subdirs:
            for subdir in sorted(subdirs):
                print(f"   - {subdir}")
        else:
            print("   (KhÃ´ng cÃ³ thÆ° má»¥c con nÃ o)")
        sys.exit(1)

    
    # Hiá»ƒn thá»‹ tá»•ng quan
    print("=" * 60)
    print(f"ğŸš€ BÃT Äáº¦U CÃ€O {len(valid_folders)} THÆ¯ Má»¤C")
    print("=" * 60)
    for i, folder in enumerate(valid_folders, 1):
        print(f"   {i}. {folder}")
    print("=" * 60)
    print()
    
    # Khá»Ÿi táº¡o scraper
    scraper = TitleScraper()
    
    # Cáº¥u hÃ¬nh delay
    delay = 1.0  # Delay 1 giÃ¢y giá»¯a cÃ¡c request
    
    # Thá»‘ng kÃª
    success_count = 0
    failed_folders = []
    
    # Xá»­ lÃ½ tá»«ng thÆ° má»¥c
    for i, folder_name in enumerate(valid_folders, 1):
        print(f"\nğŸ“ [{i}/{len(valid_folders)}] Äang xá»­ lÃ½ thÆ° má»¥c: {folder_name}")
        print()
        
        if process_folder(scraper, folder_name, delay):
            success_count += 1
        else:
            failed_folders.append(folder_name)
        
        # Delay giá»¯a cÃ¡c thÆ° má»¥c (trá»« thÆ° má»¥c cuá»‘i)
        if i < len(valid_folders):
            time.sleep(1)
    
    # Tá»•ng káº¿t
    print("\n" + "=" * 60)
    print("ğŸ“Š Tá»”NG Káº¾T")
    print("=" * 60)
    print(f"âœ… ThÃ nh cÃ´ng: {success_count}/{len(valid_folders)} thÆ° má»¥c")
    
    if failed_folders:
        print(f"âŒ Tháº¥t báº¡i: {len(failed_folders)} thÆ° má»¥c")
        for folder in failed_folders:
            print(f"   - {folder}")
    
    print("=" * 60)
    print("ğŸ‰ HOÃ€N Táº¤T Táº¤T Cáº¢!")
    print("=" * 60)


if __name__ == "__main__":
    main()