#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script cÃ o tiÃªu Ä‘á» + 100 tá»« Ä‘áº§u ná»™i dung tá»« cÃ¡c trang bÃ¡o
LÆ°u káº¿t quáº£ vÃ o data_rnn.json Ä‘á»ƒ train RNN
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import sys
import os
from typing import Dict, Tuple


class RNNScraper:
    """CÃ o tiÃªu Ä‘á» + ná»™i dung bÃ i bÃ¡o Ä‘á»ƒ train RNN"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TIÃŠU Äá»€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_title(self, soup: BeautifulSoup) -> str:
        """Láº¥y tiÃªu Ä‘á» tá»« soup object"""

        # Æ¯u tiÃªn og:title vÃ¬ thÆ°á»ng sáº¡ch hÆ¡n <title>
        og = soup.find('meta', property='og:title')
        if og and og.get('content'):
            return og['content'].strip()

        if soup.title:
            return soup.title.get_text(strip=True)

        tw = soup.find('meta', attrs={'name': 'twitter:title'})
        if tw and tw.get('content'):
            return tw['content'].strip()

        h1 = soup.find('h1')
        if h1:
            return h1.get_text(strip=True)

        return "KhÃ´ng tÃ¬m tháº¥y tiÃªu Ä‘á»"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ná»˜I DUNG â€” láº¥y 100 tá»« Ä‘áº§u cá»§a bÃ i
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_content_100_words(self, soup: BeautifulSoup) -> str:
        """
        TrÃ­ch xuáº¥t 100 tá»« Ä‘áº§u tiÃªn cá»§a ná»™i dung bÃ i bÃ¡o.
        Thá»­ nhiá»u selector phá»• biáº¿n cá»§a cÃ¡c bÃ¡o Viá»‡t Nam.
        Tráº£ vá» chuá»—i vÄƒn báº£n, KHÃ”NG pháº£i HTML.
        """

        # Danh sÃ¡ch selector theo thá»© tá»± Æ°u tiÃªn
        # Bao phá»§: VnExpress, Tuá»•i Tráº», DÃ¢n TrÃ­, Thanh NiÃªn, Znews, Tiá»n Phong...
        content_selectors = [
            # Semantic HTML5
            'article',
            # VnExpress
            'div.fck_detail',
            'div.article-body',
            # Tuá»•i Tráº»
            'div.detail-content',
            'div#main-detail-body',
            # DÃ¢n TrÃ­
            'div.singular-content',
            'div.dt-news__content',
            # Thanh NiÃªn
            'div.detail__cmain',
            'div#contentBody',
            # Znews / Zing
            'div.the-article-body',
            # Tiá»n Phong
            'div.article__body',
            # Saostar
            'div.content-detail',
            # Fallback chung
            'div.content',
            'div.post-content',
            'div.entry-content',
            'main',
        ]

        raw_text = ""

        for selector in content_selectors:
            tag, _, cls = selector.partition('.')
            if cls:
                el = soup.find(tag, class_=cls)
            else:
                # CÃ³ thá»ƒ lÃ  id hoáº·c tag Ä‘Æ¡n
                by_id = selector.partition('#')
                if by_id[1]:  # cÃ³ dáº¥u #
                    el = soup.find(by_id[0] or True, id=by_id[2])
                else:
                    el = soup.find(selector)

            if el:
                # XoÃ¡ cÃ¡c tháº» khÃ´ng liÃªn quan trong ná»™i dung
                for noise in el.find_all(['script', 'style', 'figure',
                                          'figcaption', 'aside', 'nav',
                                          'form', 'button', 'iframe']):
                    noise.decompose()

                raw_text = el.get_text(separator=' ', strip=True)
                if len(raw_text.split()) >= 20:   # cáº§n Ã­t nháº¥t 20 tá»« má»›i tÃ­nh
                    break

        # Fallback: láº¥y táº¥t cáº£ <p> trong body
        if len(raw_text.split()) < 20:
            paragraphs = soup.find_all('p')
            raw_text = ' '.join(p.get_text(strip=True) for p in paragraphs)

        # Láº¥y Ä‘Ãºng 100 tá»« Ä‘áº§u
        words = raw_text.split()
        content_100 = ' '.join(words[:100])

        return content_100 if content_100 else "KhÃ´ng tÃ¬m tháº¥y ná»™i dung"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # FETCH 1 URL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def fetch(self, url: str, timeout: int = 10) -> Tuple[str, str]:
        """
        Fetch 1 URL, tráº£ vá» (title, content_100_words)
        """
        try:
            resp = self.session.get(url, timeout=timeout)
            resp.raise_for_status()
            resp.encoding = 'utf-8'
            soup = BeautifulSoup(resp.text, 'html.parser')

            title   = self.get_title(soup)
            content = self.get_content_100_words(soup)
            return title, content

        except requests.exceptions.Timeout:
            return "Lá»—i: Timeout", ""
        except requests.exceptions.RequestException as e:
            return f"Lá»—i: {type(e).__name__}", ""
        except Exception as e:
            return f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {type(e).__name__}", ""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Äá»ŒC FILE LINKS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def read_links(self, filename: str) -> list:
        """Äá»c links tá»« file format: <url>: <tag>"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                result = []
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    idx = line.find(': ', 8)   # bá» qua "https://"
                    if idx != -1:
                        url = line[:idx].strip()
                        tag = line[idx + 1:].strip()
                    else:
                        url = line
                        tag = "KhÃ´ng cÃ³ tag"
                    result.append((url, tag))
                return result
        except FileNotFoundError:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {filename}")
            return []
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c file: {e}")
            return []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # CÃ€O TOÃ€N Bá»˜
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def scrape_all(self, input_file: str, output_file: str, delay: float = 1.0):
        """
        CÃ o táº¥t cáº£ links â†’ lÆ°u data_rnn.json

        Format output:
        {
          "https://...": {
            "title":   "TiÃªu Ä‘á» bÃ i bÃ¡o",
            "content": "100 tá»« Ä‘áº§u ná»™i dung...",
            "tag":     "giáº£i trÃ­"
          },
          ...
        }
        """
        print(f"ğŸ“‚ Äá»c links tá»«: {input_file}")
        links = self.read_links(input_file)

        if not links:
            print("âš ï¸  KhÃ´ng cÃ³ link nÃ o Ä‘á»ƒ cÃ o!")
            return

        print(f"ğŸ“ TÃ¬m tháº¥y {len(links)} links")
        print("ğŸš€ Báº¯t Ä‘áº§u cÃ o tiÃªu Ä‘á» + ná»™i dung...\n")

        # Load káº¿t quáº£ cÅ© náº¿u file Ä‘Ã£ tá»“n táº¡i (Ä‘á»ƒ resume khi bá»‹ ngáº¯t)
        results = {}
        if os.path.isfile(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"ğŸ“Œ Resume: Ä‘Ã£ cÃ³ {len(results)} bÃ i, bá» qua cÃ¡c URL trÃ¹ng\n")
            except Exception:
                results = {}

        for i, (url, tag) in enumerate(links, 1):
            # Bá» qua náº¿u Ä‘Ã£ cÃ o rá»“i
            if url in results:
                print(f"[{i}/{len(links)}] â­  Bá» qua (Ä‘Ã£ cÃ³): {url[:60]}")
                continue

            print(f"[{i}/{len(links)}] ğŸ” {url[:70]}")
            print(f"  ğŸ“Œ Tag: {tag}")

            title, content = self.fetch(url)

            results[url] = {
                "title":   title,
                "content": content,
                "tag":     tag
            }

            # Preview
            print(f"  âœ“ TiÃªu Ä‘á» : {title[:80]}")
            word_count = len(content.split())
            print(f"  âœ“ Ná»™i dung: {word_count} tá»« â€” {content[:60]}...")
            print()

            # LÆ°u ngay sau má»—i bÃ i Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u khi bá»‹ ngáº¯t
            self._save(results, output_file)

            if i < len(links):
                time.sleep(delay)

        print(f"\nâœ… HoÃ n táº¥t! ÄÃ£ lÆ°u {len(results)} bÃ i vÃ o: {output_file}")

    def _save(self, data: dict, path: str):
        """LÆ°u JSON (gá»i ná»™i bá»™ sau má»—i bÃ i)"""
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"  âš ï¸  Lá»—i lÆ°u file: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Xá»¬ LÃ THÆ¯ Má»¤C
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_folder(scraper: RNNScraper, folder: str, delay: float = 1.0) -> bool:
    input_file  = os.path.join(folder, 'links.txt')
    output_file = os.path.join(folder, 'data_rnn.json')   # â† file má»›i

    if not os.path.isfile(input_file):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y '{input_file}'")
        return False

    print("=" * 60)
    print(f"ğŸ¯ THÆ¯ Má»¤C: {folder.upper()}")
    print(f"   Input : {input_file}")
    print(f"   Output: {output_file}")
    print("=" * 60)

    scraper.scrape_all(input_file, output_file, delay)

    print("=" * 60)
    print(f"âœ¨ XONG: {folder.upper()}")
    print("=" * 60 + "\n")
    return True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    if len(sys.argv) < 2:
        print("âŒ Thiáº¿u tÃªn thÆ° má»¥c!")
        print("\nğŸ“– CÃ¡ch dÃ¹ng:")
        print("   python cao_rnn.py <thÆ°_má»¥c_1> [thÆ°_má»¥c_2] ...")
        print("\nğŸ’¡ VÃ­ dá»¥:")
        print("   python cao_rnn.py thinh")
        print("   python cao_rnn.py thinh thien huy")
        sys.exit(1)

    folder_names = sys.argv[1:]
    valid   = [f for f in folder_names if os.path.isdir(f)]
    invalid = [f for f in folder_names if not os.path.isdir(f)]

    if invalid:
        print("âš ï¸  ThÆ° má»¥c khÃ´ng tá»“n táº¡i (bá» qua):", ', '.join(invalid))

    if not valid:
        print("âŒ KhÃ´ng cÃ³ thÆ° má»¥c há»£p lá»‡!")
        sys.exit(1)

    print("=" * 60)
    print(f"ğŸš€ Báº®T Äáº¦U CÃ€O {len(valid)} THÆ¯ Má»¤C")
    print("=" * 60)

    scraper = RNNScraper()
    success = 0

    for i, folder in enumerate(valid, 1):
        print(f"\nğŸ“ [{i}/{len(valid)}] {folder}")
        if process_folder(scraper, folder):
            success += 1
        if i < len(valid):
            time.sleep(1)

    print("\n" + "=" * 60)
    print(f"ğŸ“Š Tá»”NG Káº¾T: {success}/{len(valid)} thÆ° má»¥c thÃ nh cÃ´ng")
    print("=" * 60)
    print("ğŸ‰ HOÃ€N Táº¤T!")


if __name__ == "__main__":
    main()